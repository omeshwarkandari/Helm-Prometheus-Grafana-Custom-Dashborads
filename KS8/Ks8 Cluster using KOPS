Setup Kubernetes (K8s) Cluster on AWS

Create Ubuntu EC2 instance and set-up pre-requites:
1. AWS CLI and Configure
2. KOPS Client
3. KUBECTL Client
4. IAM Access to craete Cluster (Either Create a User with permission or allocate Role to EC2 Instance)
5. Create  Hosted Zone ( Public/Private) in R53
6. Create an S3 Bucket to store Cluster config and provision
7. Genrate AWS Keys
8. Prepare Local Ennvironment: Export S3, AWS Region and Cluster Name
9. Create Cluster
10. Explore Advance Features
10. Clean-up


1. AWS CLI and AWS Configure:
$ aws --version
(version1 - aws-cli/1.18.147 Python/2.7.18 Linux/4.14.231-173.361.amzn2.x86_64 botocore/1.18.6  or  version2 - aws-cli/2.2.3 Python/3.8.8 Linux/4.14.231-173.360.amzn2...)
# In case version 1 then upgrade to the stable version 2 by downloading latest or a specfic version e.g. 2.0.30
$ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" 
or 
$ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip" -o "awscliv2.zip"
$ unzip awscliv2.zip
$ which aws (which command helps in finding symlink and the path to use Install-dir)
/usr/bin/aws 
$ ls -l /usr/bin/aws  (ls command to find the directory that your symlink points to)
lrwxrwxrwx 1 root root 31 May  9 05:14 /usr/bin/aws -> /usr/aws-cli/v2/current/bin/aws
$ sudo ./aws/install --bin-dir /usr/bin --install-dir /usr/aws-cli --update (symlink and installer info constructs the install command with the --update paramete)
$ aws --version
aws-cli/2.0.30 Python/3.7.3 Linux/4.14.231-173.361.amzn2.x86_64 botocore/2.0.0dev34


# Verify the profile availability 
$ echo $AWS_PROFILE ( It will show the default profile and if no/blank ouput then see the aws credentials section)
$ cat ~/.aws/credentials ( It will show the available cedentials and if its blank then we need to generate a profile using aws configure setup)
e.g 
[default]
aws_access_key_id = 
aws_secret_access_key = 
$ aws configure (Setup the profile as this is needed in Step 4 fro IAM User/Role creation)

 
2. Install kops on ubuntu instance:
curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x kops-linux-amd64
sudo mv kops-linux-amd64 /usr/local/bin/kops


3. Install kubectl
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl


4. IAM Access 
(In order to build clusters within AWS we'll create a dedicated IAM user for kops. This user requires API credentials in order to use kops. Create the user, 
and credentials, using the AWS console)
# In this example we will create a User e.g. kops and then create a group e.g. "kops" which will have required permissions and then add User to this Group. 
# IAM Role Permission required:
"AmazonEC2FullAccess"    "AmazonRoute53FullAccess"    "AmazonS3FullAccess"   "IAMFullAccess"   "AmazonVPCFullAccess"

# Create Group "kops" with required permissions
$ aws iam create-group --group-name kops (It will create a Json template for the group "kops")
# Now provide this Group the required access:
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops

# Create a user "kops" and add this to group as well as generate aws credentials for rest of the process.
$ aws iam create-user --user-name kops (It will create a Json template for the user "kops")
$ aws iam add-user-to-group --user-name kops --group-name kops
$ aws iam create-access-key --user-name kops

# configure the aws client to use your new IAM user:
$ aws configure
$ aws iam list-users (This will show all the users profile inclsuing current user i.e. "mukki" as well as new user "kops")

# Set-up aws config profile for "kops" by exporting the access credential.
$ cat ~/.aws/credentials ( It should show the "kops" credentials by replacing the user "mukki" credentials as default)

5. Create a Route53 private hosted zone (you can create Public hosted zone if you have a domain)
e.g. hosted zone is "demo.com" in the private dns space.
# Create using R53 Console
# Under Test: CLI (aws route53 create-hosted-zone --name example.com --caller-reference 2014-04-01-18:47 --hosted-zone-config Comment="command-line version")
$ aws route53 create-hosted-zone --name demo.com --caller-reference 2021-05-09-20:10 --hosted-zone-config Comment="command-line version"
$ aws route53 associate-vpc-with-hosted-zone --hosted-zone-id abc --vpc defaultvpc
# Hosted Zone "demo.com" in private dns created succesffully but issue in cluster creation.
# Test the hosted zone
$ dig demo.com
or
$ nslookup demo.com


6. create an S3 bucket
$ aws s3 ls   (check whether any exisitng bucket for this profile and it should be blank)
$ aws s3 mb s3://  (Create a bucket new.demo.com using mb=make bcket)
# aws s3 ls


7. Create sshkeys before creating cluster
$ ssh-keygen

8. Expose environment variable: e.g. cluster name is "cluster.demo.com" (Note: not proper local envionment creates multiple issue specially IAM Access error)
$ export AWS_ACCESS_KEY_ID=$
$ export AWS_SECRET_ACCESS_KEY=$
$ export KOPS_STATE_STORE=
$ export ZONES=us-east-1
$ export NAME=clustername


9. Create kubernetes cluser
$ kops create cluster --name= clustername \
--zones=us-east-1a \
--node-count=2 \
--dns-zone=demo.com \
--dns private

(This will create Cluster basic set-up with output to configure required set-up as belwo)

{ Must specify --yes to apply changes
Cluster configuration has been created.
Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster cluster.demo.com 
 * edit your node instance group: kops edit ig --name=cluster.demo.com nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=cluster.demo.com master-us-east-1a
Finally configure your cluster with: kops update cluster --name cluster.demo.com --yes --admin }

$ kops edit cluster <cluster-name>                                                    (To edit cluster info including name)
$ kops edit ig --name=<cluster-name> nodes-us-east-1a --state=<s3bucket-name>         (To edit the worker node configuration rather than default)
$ kops edit ig --name=<cluster-name> master-us-east-1a --state=<s3bucket-name>        (To edit the master node configuration rather than default)
$ kops update cluster --name <cluster-name> --yes --admin

# Validate your cluster
$ kops validate cluster

kubectl get nodes 
Deploying Nginx container on Kubernetes
Deploying Nginx Container

kubectl run sample-nginx --image=nginx --replicas=2 --port=80
kubectl get pods
kubectl get deployments
Expose the deployment as service. This will create an ELB in front of those 2 containers and allow us to publicly access them:

kubectl expose deployment sample-nginx --port=80 --type=LoadBalancer
kubectl get services -o wide
To delete cluster

kops delete cluster dev.k8s.valaxy.in --yes
