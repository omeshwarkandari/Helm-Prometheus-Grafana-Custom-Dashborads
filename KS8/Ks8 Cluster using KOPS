Setup Kubernetes (K8s) Cluster on AWS using KOPS (Kubernetes Operations Tool)
1) Public Cluster (Cluster in a Public VPC Subnet and directly accessible from Local System)
2) Private Cluster (Cluster in a Private VPC Subnet and accessible only through a Bastion Host created in Publcic VPC Subnet where Utility Zone means public subnet zone)
3) Private Cluster with bastion also in private subnet

Create Ubuntu EC2 instance and set-up pre-requites:
1. AWS CLI and Configure
2. KOPS Client
3. KUBECTL Client
4. IAM Access to craete Cluster (Either Create a User with permission or allocate Role to EC2 Instance)
5. Create  Hosted Zone ( Public/Private) in R53
6. Create an S3 Bucket to store Cluster config and provision
7. Genrate AWS Keys
8. Prepare Local Ennvironment: Export S3, AWS Region and Cluster Name
9. Create Cluster
10. Explore Advance Features
10. Clean-up


1. AWS CLI and AWS Configure:
$ aws --version
(version1 - aws-cli/1.18.147 Python/2.7.18 Linux/4.14.231-173.361.amzn2.x86_64 botocore/1.18.6  or  version2 - aws-cli/2.2.3 Python/3.8.8 Linux/4.14.231-173.360.amzn2...)
# In case version 1 then upgrade to the stable version 2 by downloading latest or a specfic version e.g. 2.0.30
$ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" 
or 
$ curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64-2.0.30.zip" -o "awscliv2.zip"
$ unzip awscliv2.zip
$ which aws (which command helps in finding symlink and the path to use Install-dir)
/usr/bin/aws 
$ ls -l /usr/bin/aws  (ls command to find the directory that your symlink points to)
lrwxrwxrwx 1 root root 31 May  9 05:14 /usr/bin/aws -> /usr/aws-cli/v2/current/bin/aws
$ sudo ./aws/install --bin-dir /usr/bin --install-dir /usr/aws-cli --update (symlink and installer info constructs the install command with the --update paramete)
$ aws --version
aws-cli/2.0.30 Python/3.7.3 Linux/4.14.231-173.361.amzn2.x86_64 botocore/2.0.0dev34


# Verify the profile availability 
$ echo $AWS_PROFILE ( It will show the default profile and if no/blank ouput then see the aws credentials section)
$ cat ~/.aws/credentials ( It will show the available cedentials and if its blank then we need to generate a profile using aws configure setup)
e.g 
[default]
aws_access_key_id = 
aws_secret_access_key = 
$ aws configure (Setup the profile as this is needed in Step 4 fro IAM User/Role creation)AWS
Export to use a Profile:
$ export AWS_PROFILE=<profile name>

 
2. Install kops on ubuntu instance:
curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x kops-linux-amd64
sudo mv kops-linux-amd64 /usr/local/bin/kops


3. Install kubectl
curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/local/bin/kubectl


4. IAM Access 
(In order to build clusters within AWS we'll create a dedicated IAM user for kops. This user requires API credentials in order to use kops. Create the user, 
and credentials, using the AWS console)
# In this example we will create a User e.g. kops and then create a group e.g. "kops" which will have required permissions and then add User to this Group. 
# IAM Role Permission required:
"AmazonEC2FullAccess"    "AmazonRoute53FullAccess"    "AmazonS3FullAccess"   "IAMFullAccess"   "AmazonVPCFullAccess"

# Create Group "kops" with required permissions
$ aws iam create-group --group-name kops (It will create a Json template for the group "kops")
# Now provide this Group the required access:
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops

# Create a user "kops" and add this to group as well as generate aws credentials for rest of the process.
$ aws iam create-user --user-name kops (It will create a Json template for the user "kops")
$ aws iam add-user-to-group --user-name kops --group-name kops
$ aws iam create-access-key --user-name kops

# configure the aws client to use your new IAM user:
$ aws configure
$ aws iam list-users (This will show all the users profile inclsuing current user i.e. "mukki" as well as new user "kops")

# Set-up aws config profile for "kops" by exporting the access credential.
$ cat ~/.aws/credentials ( It should show the "kops" credentials by replacing the user "mukki" credentials as default)

5. Create a Route53 private hosted zone (you can create Public hosted zone if you have a domain)
e.g. hosted zone is "demo.com" in the private dns space.
# Create using R53 Console
# Under Test: CLI (aws route53 create-hosted-zone --name example.com --caller-reference 2014-04-01-18:47 --hosted-zone-config Comment="command-line version")
$ aws ec2 describe-vpcs
$ aws route53 create-hosted-zone --name demo.com --caller-reference 1 --vpc VPCRegion=us-east-1,VPCId=vpc-498e4534
# Hosted Zone "demo.com" in private dns created succesffully but issue in cluster creation.
# Test the hosted zone
$ dig demo.com
or
$ nslookup demo.com


6. create an S3 bucket
$ aws s3 ls   (check whether any exisitng bucket for this profile and it should be blank)
$ aws s3 mb s3://  (Create a bucket new.demo.com using mb=make bcket)
# aws s3 ls


7. Create sshkeys before creating cluster
$ ssh-keygen

8. Expose environment variable: e.g. cluster name is "cluster.demo.com" (Note: not proper local envionment creates multiple issue specially IAM Access error)
$ export AWS_ACCESS_KEY_ID=$
$ export AWS_SECRET_ACCESS_KEY=$
$ export KOPS_STATE_STORE=
$ export ZONES=us-east-1
$ export NAME=mycluster

Set-up Public Cluster:
9. Create kubernetes cluser
$ kops create cluster --name= mycluster.dem.com \
--zones=us-east-1a \
--node-count=2 \
--dns-zone=demo.com \
--dns private

(This will create Cluster basic set-up with output to configure required set-up as belwo)

{ Must specify --yes to apply changes
Cluster configuration has been created.
Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster cluster.demo.com 
 * edit your node instance group: kops edit ig --name=cluster.demo.com nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=cluster.demo.com master-us-east-1a
 In case of HA we can craete master and nodes in mutlriple AZ's for resilency but it has additonal cost both in terms of EC2 Resource for Master but also you pay for the
 cross-AZ traffic. Since the kubernetes etcd cluster runs on the master nodes, you have to specify an odd number of zones in order to obtain quorum.
 $ kops create cluster --name= mycluster.demo.com --zones=us-east-1a,us-east-1c,us-east-1b --node-count=3 --master-zones=us-east-1a,us-east-1c,us-east-1b
 --dns-zone=demo.com --dns private
Finally configure your cluster with: kops update cluster --name cluster.demo.com --yes --admin }
$ kops edit cluster <cluster-name>                                                    (To edit cluster info including name)
$ kops edit ig --name=<cluster-name> nodes-us-east-1a --state=<s3bucket-name>         (To edit the worker node configuration rather than default)
$ kops edit ig --name=<cluster-name> master-us-east-1a --state=<s3bucket-name>        (To edit the master node configuration rather than default)
$ kops update cluster --name <cluster-name> --yes --admin
# Validate your cluster
$ kops validate cluster
#Additional Commands once cluster edited post creation
$ kops update cluster --name <cluster-name> --state <bucket name> --yes
$ kops rolling-update cluster --state <bucket name> --yes --cloudonly

kubectl get nodes 
Deploying Nginx container on Kubernetes
Deploying Nginx Container

kubectl run sample-nginx --image=nginx --replicas=2 --port=80
kubectl get pods
kubectl get deployments
Expose the deployment as service. This will create an ELB in front of those 2 containers and allow us to publicly access them:

kubectl expose deployment sample-nginx --port=80 --type=LoadBalancer
kubectl get services -o wide



# Private Cluster
$ export ZONES=us-east-1
$ export KOPS_STATE_STORE=s3://bucket.demo.com
$export NAME=mycluster.demo.com
Craete Cluster with Bastion Host in Public Subnet:
$ kops create cluster --node-count 2 --node-size t2.micro --master-size t3.medium --zones us-east-1a --master-zones us-east-1a --dns-zone demo.com --topology private 
--networking calico --dns priv

$ export KOPS_STATE_STORE=s3://bucket.demo.comate --bastion ${NAME}
$ kops create instancegroup bastions --role Bastion --subnet utility-us-east-1a --name mycluster.demo.com
$ kops update cluster --name mycluster.demo.com
$ kops rolling-update cluster --name mycluster.demo.com
# This will create a Bastion Host in bastion public subnet as well as ELB for our "bastions" instance group that we can check with the following command:
$aws elb --output=table describe-load-balancers|grep DNSName.\*bastion|awk '{print $4}'
$ ssh -i ~/.ssh/id_rsa ubuntu@ ELB DNS Name  or $ ssh -A ubuntu@ ELB DNS Name
# To enable Bation host ssh into Cluster Nodes we need to follow below steps with the explanation shared in the document SSH Agent
$ ssh-agent
$ eval "$(ssh-agent)" or eval `ssh-agent -s` 
$ ssh-add ~/.ssh/id_rsa
$ ssh-add -l
Identity added: /home/kops/.ssh/id_rsa (/home/kops/.ssh/id_rsa), Now ssh to your bastion ELB FQDN and from bastion host again ssh into the Cluster Node with its Private DNS/IP
$ ssh -A ubuntu@ ELB DNS Name e.g. $ ssh -A ubuntu@bastion-privatekOpscluste-bgl0hp-1327959377.us-east-1.elb.amazonaws.com
ubuntu@ip-172-20-2-64:~$ ssh ubuntu@ Master Node Private DNS
# We can automate the SSH Into Bastion
$ ssh -A ubuntu@`aws elb --output=table describe-load-balancers|grep DNSName.\*bastion|awk '{print $4}'`

# Create Cluster with Bastion Host in Private Subnet:
$ kops create cluster --node-count 2 --node-size t2.micro --master-size t2.micro --zones us-east-1a --master-zones us-east-1a --dns-zone demo.com
--topology private --networking calico --dns private --bastion ${NAME}

